# ğŸ§  Voice-Enabled AI Chatbot with PDF & Web Knowledge using LangChain, AstraDB, and Ollama

## ğŸš€ Overview

This project implements a **voice-enabled AI chatbot** that supports **natural language querying** over both:

- **PDF documents**
- **Web-based content** (e.g., Wikipedia)

Built using **LangChain**, **Ollama with LLaMA 3**, and **vector stores** (AstraDB + ChromaDB), the chatbot provides **retrieval-augmented generation (RAG)** with **seamless voice interaction** via STT (Speech-to-Text) and TTS (Text-to-Speech).

---

## ğŸ§© Features

- ğŸ“„ **PDF Knowledge Retrieval**: Extracts and queries PDFs with LLaMA 3 and AstraDB.  
- ğŸŒ **Web Data Retrieval**: Loads content from URLs and retrieves answers using ChromaDB.  
- ğŸ—£ï¸ **Voice Interaction**: Talk to the chatbot via microphone; it responds with speech.  
- ğŸ§  **Locally Hosted LLM**: Uses **Ollama** to run **LLaMA 3** model locally (no API cost).  
- ğŸ” **LangChain Integration**: Leverages advanced RAG pipeline for accurate, context-aware answers.  
- ğŸ”Š **Realistic Text-to-Speech**: Natural-sounding voice output using Edge TTS and PyDub.  
- ğŸ§ **Cross-platform Audio Playback**: Uses `sounddevice` for interactive experience.  

---

## ğŸ› ï¸ Tech Stack

| Feature                  | Technology Used                         |
|--------------------------|------------------------------------------|
| AI Engine (LLM)          | [Ollama](https://ollama.ai) (LLaMA 3)    |
| AI Pipeline              | LangChain + RetrievalQA                 |
| Vector Store (PDFs)      | [AstraDB](https://www.datastax.com/astra)|
| Vector Store (Web Data)  | ChromaDB                                |
| Voice Input (STT)        | SpeechRecognition                       |
| Voice Output (TTS)       | Edge TTS + PyDub + sounddevice          |
| PDF Parsing              | PyPDF2                                  |
| Web Data Scraping        | Wikipedia & WebLoader                   |
| Frontend                 | HTML + JS (for voice input/output)      |
| Backend APIs             | Flask (Chatbot UI) + FastAPI (Voice API)|

---

## ğŸ“‚ Project Structure

```
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ app.py                # Flask UI backend
â”‚   â”œâ”€â”€ index.html            # Simple web UI
â”‚   â”œâ”€â”€ style.css             # UI styling
â”œâ”€â”€ voice_support/
â”‚   â”œâ”€â”€ audioServer.py        # FastAPI Voice-to-Text & TTS
â”œâ”€â”€ pdf_engine/
â”‚   â”œâ”€â”€ pdf_processor.py      # Extracts and embeds PDF into AstraDB
â”œâ”€â”€ web_engine/
â”‚   â”œâ”€â”€ web_loader.py         # Loads and embeds web data into ChromaDB
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ speech_utils.py       # Voice input/output utilities
â”œâ”€â”€ cert.pem / key.pem        # SSL certificates for HTTPS
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## âš™ï¸ Installation

### 1. Clone the Repository
```bash
git clone https://github.com/your-username/your-repo.git
cd your-repo
```

### 2. Install Dependencies
Ensure Python 3.9+ is installed.

```bash
pip install -r requirements.txt
```

---

## ğŸ”§ Setup Instructions

### Step 1: Set Environment Variables

#### For AstraDB (PDF Support)
```bash
export ASTRA_DB_APPLICATION_TOKEN='your_astra_token'
export ASTRA_DB_ID='your_astra_db_id'
export ASTRA_DB_API_ENDPOINT='https://your-db-api-id.apps.astra.datastax.com'
```

#### Optional: For Ollama (Local)
```bash
ollama pull llama3
ollama run llama3
```

#### OR Use Groq API (Remote LLaMA)
```bash
export GROQ_API_KEY='your_groq_api_key'
```

---

## â–¶ï¸ Running the Application

### Step 1: Launch Flask UI Backend
```bash
cd app
python app.py
```
Runs on: `http://localhost:5000`

### Step 2: Launch FastAPI Voice Server
```bash
cd voice_support
uvicorn audioServer:app --host 0.0.0.0 --port 8000
```
Runs on: `http://localhost:8000`

### Step 3: Launch Frontend
```bash
python -m http.server 5500 --bind 0.0.0.0
```
Then open: `http://localhost:5500/index.html`

âœ… You can now talk to your chatbot via the browser.

---

## ğŸ“˜ How It Works

### ğŸ“„ PDF Querying (via AstraDB)
1. Load and chunk text using PyPDF2.  
2. Embed chunks using LangChain and store in AstraDB.  
3. Use LangChainâ€™s `RetrievalQA` to find answers from PDF embeddings.  

### ğŸŒ Web Querying (via ChromaDB)
1. Extract content using LangChain's WebLoader (Wikipedia/web pages).  
2. Chunk and embed with ChromaDB as vector store.  
3. Query responses using LLaMA 3 via LangChain Retriever.  

### ğŸ—£ï¸ Voice Interaction
- User speaks via microphone â†’ `speech_recognition` converts to text.  
- Query is passed to LangChain pipeline (PDF or Web).  
- LLaMA 3 generates response.  
- Response is converted to speech via `edge-tts`.  
- Audio is played with `sounddevice`.  

---

## ğŸ§ª Example Interactions

**PDF-based**
```
User: "Explain the MVC architecture mentioned in the document."
Bot: "The Model-View-Controller architecture separates application logic into three components..."
```

**Web-based**
```
User: "Tell me about the history of Python programming."
Bot: "Python was created by Guido van Rossum in the late 1980s and released in 1991..."
```

---

## ğŸŒ± Future Enhancements

- âœ… Multi-document PDF support  
- âœ… Web + PDF hybrid querying  
- ğŸŒ HTTPS support via SSL certificates  
- ğŸ§  Continuous learning from user interaction  
- ğŸ“± Mobile & PWA frontend for universal access  
- ğŸ—£ï¸ Multilingual voice input and output  
